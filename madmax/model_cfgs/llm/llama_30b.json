{
    "name" : "LLaMA_30B",
    "type" : "LLM",
    "bytes_per_nonemb_param" : 2,
    "bytes_per_emb_param" : 2,
    "entries_per_table" : 50257,
    "num_transformer_layers" : 60,
    "num_transformer_heads" : 52,
    "attention_dim" : 6656,
    "transformer_fc_dim" : 17750,
    "transformer_seq_len" : 2048
}